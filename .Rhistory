train <- read.csv('./lottery.csv', stringsAsFactors = F)
# STEP 1: Split the data set into two parts: 'train' and 'test'
# your train data should be more 70% of the given data set
# show your code below
set.seed(2023) # Seed for Randomization
# This Seed Value will be also used for random forest.
# split_index: index for split data in train data and test data.
# sample will generate a random vector for index.
# Using nrow(), split_index will be able to split data as the fraction (0.75) (75%) based on the given data.
split_index <- sample(seq_len(nrow(train)), size = nrow(train) * 0.75)
# Split Dataset
test <- train[-split_index, ]
train <- train[split_index, ]
# Display your datasets. Do Not Delete.
str(train)
str(test)
# Build a random forest mode
# STEP 2-1: Set a random seed
# show your code below
set.seed(613) # Seed for random forest
# STEP 2-2: Build your random forest model
# Selected Feature: first, second, third, fourth, fifth, sixth, bonus
# Customized Feature: Winnable Range
# Within_Winnable_Range - Boolean: Show if current row contains more than three numbers resides in winnable range.
# Including the bonus numbers, the number distribution shows.
#   1   1~9  1498
#   2 10~19  1734
#   3 20~29  1618
#   4 30~39  1706
#   5 40~45  1004
# This shows that the number in range 10~19 and 30~39 shows the high distribution of winning numbers.
# Define those two range (10~19, 30~39) as a 'winnable range'
# If the row contains more than 3 values within the winnable range, this feature will show 'true' value.
train <- train %>%
rowwise() %>%
mutate(
"within_winnable_range" = sum(c_across(all_of(c("first","second","third","fourth","fifth","sixth","bonus"))) %in% 10:19) +
sum(c_across(all_of(c("first","second","third","fourth","fifth","sixth","bonus"))) %in% 30:39) > 3
)
train$win = as.factor(train$win) # Convert predictor variable (Independent Variable) into factor
RF_lottery_model <- randomForest(win ~ first+second+third+fourth+fifth+sixth+bonus+within_winnable_range,
data = train, ntree = 500, importance = TRUE)
# Show model error, calculate importance, and rank valuable. Draw figures.
# STEP 3: Show model error and plot it.
# show your code below
# Make a prediction based on the trained model
# Create 'within_winnable_range' feature to test data.
test <- test %>%
rowwise() %>%
mutate(
"within_winnable_range" = sum(c_across(all_of(c("first","second","third","fourth","fifth","sixth","bonus"))) %in% 10:19) +
sum(c_across(all_of(c("first","second","third","fourth","fifth","sixth","bonus"))) %in% 30:39) > 3
)
test$win <- as.factor(test$win)
# Make a prediction
# Prediction using your_model against test data. Do Not Delete.
prediction <- predict(RF_lottery_model, test)
actual <- test$win
# 1. Calculate Model Error; Show the Accuracy.
#accuracy <- sum(actual == prediction) / length(actual) # Accuracy = Corrected / Total Case
# Calculate Accuracy using Confusion Matrix
confusion_matrix = table(actual, prediction)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix) # TP + TN / (TP + TN + FP + FN)
model_error <- 1 - accuracy
cat("Accuracy for current RF model: ", round(accuracy, 3),"\n")
cat("Model Error: ", round(model_error, 3), "\n")
# 2. Calculate the feature importance, and rank them.
randomForest::importance(RF_lottery_model)
randomForest::varImpPlot(RF_lottery_model)
# 3. Plot the model, draw figures.
# Append Predicted, and Actual Result in test data set.
# Plot the Model Prediction and actual values
# X Axis: Shows the actual value of current case.
# Y Axis: Shows the predicted value of current case.
predicted_prob = data.frame(predict(RF_lottery_model, test, type="prob"))
test_res <- test
test_res['prob'] <- predicted_prob$X1
ggplot() +
xlab("Actual Value") +
ylab("Predicted Probability") +
geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
# STEP 4: Calculate solution (using prediction above) and save it to 'lottery-solution.csv' where it contain the two columns: round and win.
# show your code below
solution <- data.frame(
round = test$round,
win = prediction
)
write.csv(solution, "./lottery-solution.csv", row.names = FALSE)
test_res['predicted'] <- prediction
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(aes(x = test_res$win, y = test_res$predicted, color='Prediction'))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(aes(x = test_res$rounds, y = test_res$predicted, color='Prediction'))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(aes(x = test_res$round, y = test_res$predicted, color='Prediction'))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(aes(x = test_res$predicted, y = test_res$round, color='Prediction'))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(aes(x = test_res$round, y = test_res$predicted, color='Prediction'))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(aes(x = test_res$round, y = test_res$prob, color='Prediction'))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(aes(x = test_res$prob, y = test_res$win, color='Prediction'))
####### How to install R using Homebrew:
# $brew install r or $brew cask install rstudio
# $sudo r
####### How to install R packages:
# $sudo r
# $install.packages("gplot2")
####### How to run R script:
# $Rscript your-file.R
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('randomForest') # classification algorithm
# NOTE: Take a good look at the sample code that we had in class. I hope it helps.
# Reading the data set (lottery-train.csv)
train <- read.csv('./lottery.csv', stringsAsFactors = F)
# STEP 1: Split the data set into two parts: 'train' and 'test'
# your train data should be more 70% of the given data set
# show your code below
set.seed(2023) # Seed for Randomization
# This Seed Value will be also used for random forest.
# split_index: index for split data in train data and test data.
# sample will generate a random vector for index.
# Using nrow(), split_index will be able to split data as the fraction (0.75) (75%) based on the given data.
split_index <- sample(seq_len(nrow(train)), size = nrow(train) * 0.75)
# Split Dataset
test <- train[-split_index, ]
train <- train[split_index, ]
# Display your datasets. Do Not Delete.
str(train)
str(test)
# Build a random forest mode
# STEP 2-1: Set a random seed
# show your code below
set.seed(613) # Seed for random forest
# STEP 2-2: Build your random forest model
# Selected Feature: first, second, third, fourth, fifth, sixth, bonus
# Customized Feature: Winnable Range
# Within_Winnable_Range - Boolean: Show if current row contains more than three numbers resides in winnable range.
# Including the bonus numbers, the number distribution shows.
#   1   1~9  1498
#   2 10~19  1734
#   3 20~29  1618
#   4 30~39  1706
#   5 40~45  1004
# This shows that the number in range 10~19 and 30~39 shows the high distribution of winning numbers.
# Define those two range (10~19, 30~39) as a 'winnable range'
# If the row contains more than 3 values within the winnable range, this feature will show 'true' value.
train <- train %>%
rowwise() %>%
mutate(
"within_winnable_range" = sum(c_across(all_of(c("first","second","third","fourth","fifth","sixth","bonus"))) %in% 10:19) +
sum(c_across(all_of(c("first","second","third","fourth","fifth","sixth","bonus"))) %in% 30:39) > 3
)
train$win = as.factor(train$win) # Convert predictor variable (Independent Variable) into factor
RF_lottery_model <- randomForest(win ~ first+second+third+fourth+fifth+sixth+bonus+within_winnable_range,
data = train, ntree = 500, importance = TRUE)
# Show model error, calculate importance, and rank valuable. Draw figures.
# STEP 3: Show model error and plot it.
# show your code below
# Make a prediction based on the trained model
# Create 'within_winnable_range' feature to test data.
test <- test %>%
rowwise() %>%
mutate(
"within_winnable_range" = sum(c_across(all_of(c("first","second","third","fourth","fifth","sixth","bonus"))) %in% 10:19) +
sum(c_across(all_of(c("first","second","third","fourth","fifth","sixth","bonus"))) %in% 30:39) > 3
)
test$win <- as.factor(test$win)
# Make a prediction
# Prediction using your_model against test data. Do Not Delete.
prediction <- predict(RF_lottery_model, test)
actual <- test$win
# 1. Calculate Model Error; Show the Accuracy.
#accuracy <- sum(actual == prediction) / length(actual) # Accuracy = Corrected / Total Case
# Calculate Accuracy using Confusion Matrix
confusion_matrix = table(actual, prediction)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix) # TP + TN / (TP + TN + FP + FN)
model_error <- 1 - accuracy
cat("Accuracy for current RF model: ", round(accuracy, 3),"\n")
cat("Model Error: ", round(model_error, 3), "\n")
# 2. Calculate the feature importance, and rank them.
randomForest::importance(RF_lottery_model)
randomForest::varImpPlot(RF_lottery_model)
# 3. Plot the model, draw figures.
# Append Predicted, and Actual Result in test data set.
# Plot the Model Prediction and actual values
# X Axis: Shows the actual value of current case.
# Y Axis: Shows the predicted value of current case.
predicted_prob = data.frame(predict(RF_lottery_model, test, type="prob"))
test_res <- test
test_res['prob'] <- predicted_prob$X1
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(aes(x = test_res$prob, y = test_res$win, color='Prediction'))
# STEP 4: Calculate solution (using prediction above) and save it to 'lottery-solution.csv' where it contain the two columns: round and win.
# show your code below
solution <- data.frame(
round = test$round,
win = prediction
)
write.csv(solution, "./lottery-solution.csv", row.names = FALSE)
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(aes(x = test_res$prob, y = test_res$win, color='Prediction')) +
geom_smooth(aes(method="glm", x = test_res$prob, y = test_res$win))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point() +
geom_smooth(aes(method="glm", x = test_res$prob, y = test_res$win))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point() +
geom_smooth(aes(method="glm", method.args = list(family="binomial") x = test_res$prob, y = test_res$win))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point() +
geom_smooth(aes(method="glm", method.args = list(family="binomial") x = test_res$prob, y = test_res$win))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point() +
geom_smooth(aes(method="glm", method.args = list(family="binomial") x = test_res$prob, y = test_res$round))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point() +
geom_smooth(aes(method="glm", method.args = list(family="binomial"), x = test_res$prob, y = test_res$round))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point() +
geom_smooth(aes(method="glm", method.args = list(family="binomial"), x = test_res$prob, y = test_res$win))
round
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot() +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point() +
geom_smooth(aes(method="glm", method.args = list(family="binomial"), x = test_res$prob, y = test_res$round))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot(aes(x = test_res$prob, y = test_res$round)) +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point() +
geom_smooth(aes(method="glm", method.args = list(family="binomial")))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot(x = test_res$prob, y = test_res$round) +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point() +
geom_smooth(aes(method="glm", method.args = list(family="binomial")))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot(test_tes, aes(x = prob, y = round)) +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point() +
geom_smooth(aes(method="glm", method.args = list(family="binomial")))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot(test_res, aes(x = prob, y = round)) +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point() +
geom_smooth(aes(method="glm", method.args = list(family="binomial")))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot(test_res, aes(x = round, y = prob)) +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point() +
geom_smooth(aes(method="glm", method.args = list(family="binomial")))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot(test_res, aes(x = round, y = prob)) +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(colors="magenta") +
geom_smooth(aes(method="glm", method.args = list(family="binomial")))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot(test_res, aes(x = round, y = prob)) +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(color = 'magenta')) +
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot(test_res, aes(x = round, y = prob)) +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(color = 'magenta') +
geom_smooth(aes(method="glm", method.args = list(family="binomial")))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot(test_res, aes(x = round, y = prob)) +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(color = 'magenta', alpha=0.5) +
geom_smooth(aes(method="glm", method.args = list(family="binomial")))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot(test_res, aes(x = round, y = prob)) +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(color = 'magenta', alpha(0.5)) +
geom_smooth(aes(method="glm", method.args = list(family="binomial")))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot(test_res, aes(x = round, y = prob)) +
xlab("Actual Value") +
ylab("Predicted Value") +
geom_point(color = 'red') +
geom_smooth(aes(method="glm", method.args = list(family="binomial")))
#ggplot() +
#  xlab("Actual Value") +
#  ylab("Predicted Probability") +
#  geom_jitter(aes(x = test_res$win, y = test_res$prob, color='Prediction'))
ggplot(test_res, aes(x = round, y = prob)) +
xlab("Actual Value") +
ylab("Predicted Probability") +
geom_point(color = 'red') +
geom_smooth(aes(method="glm", method.args = list(family="binomial")))
# AI-X Final Project
# Unsupervised Learning based on TF-IDF
# Dataset: League of Legends Tribunal Chatlogs (Kaggle)
# https://www.kaggle.com/datasets/simshengxue/league-of-legends-tribunal-chatlogs
library(dplyr) # R Package: dplyr - advanced filtering and selection
library(tm) # R Package: tm - Text Mining/Merging for preprocess of TF-IDF, and TF-IDF itself
setwd("~/Desktop/Dev/HYU/2023-02/AI-X/project/gamechatban") # Change this value to your working dir.
chatlogs <- read.csv("./chatlogs.csv")
# Pre-Processing Steps; Feature Engineering Pipeline for the chatlogs.
# 1. Grammatical & Game-specific Expression Removal:
# Remove common grammatical expressions like "is" and "are" to enhance the validity of the analysis.
# 2. Feature Engineering - Severity:
# Introduce a new feature called 'severity' based on the total number of case reports.
# Total case report <= 3: Severe
# Total Case Report >= 4 && <= 6: Normal
# Total Case Report >= 7: Low
# 3. Concatenation of Chatlogs:
# Group chatlogs based on the common reported reason.
# Concatenate chatlogs within each group into a single text.
# Chatlogs will be merged into a single column for each most common reported reason, considering the newly defined 'severity' feature.
# Step 1: Gramatical Game-specific Expression Removal: Used gsub and REGEX to do such task.
# Read champion names
champion_names <- read.csv("./champion_names.csv")
# Create a regex pattern for both grammatical expressions and champion names/abbreviations
pattern <- paste0("\\b(?:is|are|&gt|&lt|was|were|", paste(unique(c(champion_names$Champion, champion_names$Abbreviation)), collapse = "|"), ")\\b")
# Remove both grammatical expressions and champion names/abbreviations from chatlogs$message
chatlogs$message <- gsub(pattern, "", chatlogs$message, ignore.case = TRUE)
# Step 2:  Feature Engineering - Severity
chatlogs$severity <- cut( # Categorize numbers into factors.
chatlogs$case_total_reports,
breaks = c(-Inf, 3, 6, Inf),
labels = c("Severe", "Normal", "Low"),
include.lowest = TRUE
)
# Step 3: Concatenation of Chatlogs
concatenated <- chatlogs %>%
group_by(most_common_report_reason, severity) %>% #Group by following two category
summarise(concatenated_text = paste(message, collapse = " ")) %>%
ungroup()
# TF-IDF (Term-Frequency Inverse Document Frequency) Matrix Anaylsis; Process TF-IDF for each concatenated text to get 'toxiticy level of each words'.
# 1. Create a corpus for TF-IDF, pre-process it.
# 2. Create DTM for TF-IDF, and generate TF-IDF matrix
# 3. Transpose it and apply new column name to analyse the reported reason and severity.
# 4. Scale Up and round the value to get toxic level.
# 5. Export into csv. 'toxicity_lev.csv'
# Create a Corpus from the column concatenated_text.
corpus <- Corpus(VectorSource(concatenated$concatenated_text))
# Additional pre-process of the text in the corpus. (e.g. removing punctuation, stripping whitespaces, etc.)
corpus <- tm_map(corpus, content_transformer(tolower)) # Convert each contents into lower case
corpus <- tm_map(corpus, removePunctuation) # Remove Punctuations
corpus <- tm_map(corpus, removeWords, stopwords("english")) # Remove Additional English stopwords (a, the, etc) that hadn't been filtered.
corpus <- tm_map(corpus, stripWhitespace) # Strip Whitespace
# Create DTM (Document-Term Matrix) based on the corpus, which is used for TF-IDF
dtm <- DocumentTermMatrix(corpus)
# Create TF-IDF Matrix based on the DTM.
tf_idf <- weightTfIdf(dtm)
tf_idf <- t(as.matrix(tf_idf)) # Transpose
# Generate Column name
tf_idf_col_name <- paste(concatenated$most_common_report_reason, concatenated$severity, sep = "_")
# Set the column name of the transposed TF_IDF
colnames(tf_idf) <- tf_idf_col_name
# Scale up and Round the values
tf_idf <- round((tf_idf * 10000), 2)
# Convert TF-IDF matrix into a new data frame for further analysis.
tf_idf_df <- as.data.frame(tf_idf)
# Make it into csv for further analysis & supervised learning.
write.csv(tf_idf_df, "toxicity_lev.csv")
View(tf_idf_df)
View(tf_idf_df)
# Export into csv for later use. (Pre-processed.csv)
write.csv(chatlogs, "processed.csv")
