geom_smooth(aes(method="glm", method.args = list(family="binomial")))
library(randomForest) # R Package: Random Forest Model
library(keras) # R Package: LSTM Model
library(caret) # R Package:
library(tensorflow) # R Package: Tensorflow
library(reticulate) # R Package for interfacing with python
setwd("~/Desktop/Dev/HYU/2023-02/AI-X/project/gamechatban") # Change this value to your working dir.
#use_virtualenv("/Users/kyumericano/.virtualenvs/r-reticulate/bin/python", required = TRUE)
reticulate::py_config() # Show python interpreter currently configured to use.
# Read a dataset for supervised learning.
processed_df <- read.csv('./offender_chatlog_with_toxic_score.csv')
processed_df <- processed_df[complete.cases(processed_df$toxic_score), ] # Remove rows with missing values
# 1-1 Feature Angineering
# use 'message' column as the feature
messages <- processed_df$message
# Tokenize the message
tokenizer <- text_tokenizer() # Tokenizes text into individual words.
fit_text_tokenizer(tokenizer, messages) # Fits the tokenizer on the input text 'message'
word_idx <- tokenizer$word_index # Retrieves the word index generated by the tokenizer.
sequences <- texts_to_sequences(tokenizer, messages) # Converts the text to a sequence of word indices.
View(tokenizer)
View(processed_df)
View(sequences)
View(sequences)
# AI-X Final Project
# Supervised Learning after doing unsupervised learning.
# Dataset: A chatlog with toxic score assessed by unsupervised learning.
# Column: X(id), Message, Most common report reason, Toxic Score
library(randomForest) # R Package: Random Forest Model
library(keras) # R Package: LSTM Model
library(caret) # R Package:
library(tensorflow) # R Package: Tensorflow
library(reticulate) # R Package for interfacing with python
setwd("~/Desktop/Dev/HYU/2023-02/AI-X/project/gamechatban") # Change this value to your working dir.
#use_virtualenv("/Users/kyumericano/.virtualenvs/r-reticulate/bin/python", required = TRUE)
reticulate::py_config() # Show python interpreter currently configured to use.
# Read a dataset for supervised learning.
processed_df <- read.csv('./offender_chatlog_with_toxic_score.csv')
processed_df <- processed_df[complete.cases(processed_df$toxic_score), ] # Remove rows with missing values
# We are trying to achieve two objective by following supervised learning.
# 1.Build a model that can predict (regress) the toxic level of message.
# 2.Build a model that can classify the most possible common report reason of following chat.
# Objective 1. [Regression Task]: Build a model that can predict the toxic level of message.
# Strategy
# 1. Feature Engineering
# Extract message and toxic score, which are the features required for learning.
# Split data into training data and test data (75% : 25%)
# 2. Build a regression model using random forest model.
# Make regression and evaluate the model.
# Plot the model result.
# 3. Build a regression model using LSTM Model
# Make regression and evaluate the model.
# Plot the model result.
# 1-1 Feature Angineering
# use 'message' column as the feature
messages <- processed_df$message
# Tokenize the message
tokenizer <- text_tokenizer() # Tokenizes text into individual words.
fit_text_tokenizer(tokenizer, messages) # Fits the tokenizer on the input text 'message'
word_idx <- tokenizer$word_index # Retrieves the word index generated by the tokenizer.
sequences <- texts_to_sequences(tokenizer, messages) # Converts the text to a sequence of word indices.
padded_sequences <- pad_sequences(sequences, maxlen = 50) # Pads sequences to have a consistent length, and it returns a matrix where each row corresponds to a padded sequence.
# Split the data
set.seed(sample(100:1000,1,replace=F)) # Random sampling
trainIndex <- createDataPartition(processed_df$toxic_score, p = 0.75 ,list = FALSE)
train_data <- padded_sequences[trainIndex, ]
test_data <- padded_sequences[-trainIndex, ]
# 1-2: Build a regression model using random forest model. (Random Forest Regressor)
# Random Forest Model
#rf_model <- randomForest(toxic_score ~ ., data = processed_df[trainIndex, c("toxic_score", "message")], ntree = 100)
# Make predictions
#predictions_rf <- predict(rf_model, newdata = processed_df[-trainIndex, c("toxic_score", "message")])
# Evaluate the model
# Evaluate the model via MSE (Mean Squared error)
#mse_rf <- mean((processed_df$toxic_score[-trainIndex] - predictions_rf)^2)
#cat("Random Forest Mean Squared Error:", mse_rf, "\n")
# 1-3: Build a regression model using LSTM model. (Long Short Term Memory Model)
vocab_size <- length(word_idx) # 43268L
seq_num <- ncol(padded_sequences)
# Define the LSTM model
model_lstm <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size, output_dim = 50, input_length = seq_num) %>%
layer_lstm(units = 64, activation = "relu") %>% # Apply ReLU as an activation function to prevent gradient vanishing problem.
layer_dense(units = 1)  # Output layer with a single node for regression
# Compile the model
model_lstm %>% compile(
optimizer = optimizer_adam(clipnorm = 1.0), # Used keras adam oprimizer. Applied clipnorm to limit the gradient value.
loss = "mean_squared_error",  # Simple MSE as a loss function (uniform loss with random forest.)
metrics = c("mean_squared_error")
)
# Check the model summary
summary(model_lstm)
# Check the input data shape
cat("Input Data Shape:", dim(train_data), "\n")
# Train the LSTM Model
model_lstm %>% fit(
x = padded_sequences[trainIndex, , drop = FALSE],
y = processed_df$toxic_score[trainIndex],
epochs = 10,
batch_size = 32,
validation_split = 0.2
)
# AI-X Final Project
# Supervised Learning after doing unsupervised learning.
# Dataset: A chatlog with toxic score assessed by unsupervised learning.
# Column: X(id), Message, Most common report reason, Toxic Score
library(randomForest) # R Package: Random Forest Model
library(keras) # R Package: LSTM Model
library(caret) # R Package:
library(tensorflow) # R Package: Tensorflow
library(reticulate) # R Package for interfacing with python
setwd("~/Desktop/Dev/HYU/2023-02/AI-X/project/gamechatban") # Change this value to your working dir.
#use_virtualenv("/Users/kyumericano/.virtualenvs/r-reticulate/bin/python", required = TRUE)
reticulate::py_config() # Show python interpreter currently configured to use.
# Read a dataset for supervised learning.
processed_df <- read.csv('./offender_chatlog_with_toxic_score.csv')
processed_df <- processed_df[complete.cases(processed_df$toxic_score), ] # Remove rows with missing values
# We are trying to achieve two objective by following supervised learning.
# 1.Build a model that can predict (regress) the toxic level of message.
# 2.Build a model that can classify the most possible common report reason of following chat.
# Objective 1. [Regression Task]: Build a model that can predict the toxic level of message.
# Strategy
# 1. Feature Engineering
# Extract message and toxic score, which are the features required for learning.
# Split data into training data and test data (75% : 25%)
# 2. Build a regression model using random forest model.
# Make regression and evaluate the model.
# Plot the model result.
# 3. Build a regression model using LSTM Model
# Make regression and evaluate the model.
# Plot the model result.
# 1-1 Feature Angineering
# use 'message' column as the feature
messages <- processed_df$message
# Tokenize the message
tokenizer <- text_tokenizer() # Tokenizes text into individual words.
fit_text_tokenizer(tokenizer, messages) # Fits the tokenizer on the input text 'message'
word_idx <- tokenizer$word_index # Retrieves the word index generated by the tokenizer.
sequences <- texts_to_sequences(tokenizer, messages) # Converts the text to a sequence of word indices.
padded_sequences <- pad_sequences(sequences, maxlen = 20) # Pads sequences to have a consistent length, and it returns a matrix where each row corresponds to a padded sequence.
# Split the data
set.seed(sample(100:1000,1,replace=F)) # Random sampling
trainIndex <- createDataPartition(processed_df$toxic_score, p = 0.75 ,list = FALSE)
train_data <- padded_sequences[trainIndex, ]
test_data <- padded_sequences[-trainIndex, ]
# 1-2: Build a regression model using random forest model. (Random Forest Regressor)
# Random Forest Model
#rf_model <- randomForest(toxic_score ~ ., data = processed_df[trainIndex, c("toxic_score", "message")], ntree = 100)
# Make predictions
#predictions_rf <- predict(rf_model, newdata = processed_df[-trainIndex, c("toxic_score", "message")])
# Evaluate the model
# Evaluate the model via MSE (Mean Squared error)
#mse_rf <- mean((processed_df$toxic_score[-trainIndex] - predictions_rf)^2)
#cat("Random Forest Mean Squared Error:", mse_rf, "\n")
# 1-3: Build a regression model using LSTM model. (Long Short Term Memory Model)
vocab_size <- length(word_idx) # 43268L
seq_num <- ncol(padded_sequences)
# Define the LSTM model
model_lstm <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size, output_dim = 50, input_length = seq_num) %>%
layer_lstm(units = 64, activation = "relu") %>% # Apply ReLU as an activation function to prevent gradient vanishing problem.
layer_dense(units = 1)  # Output layer with a single node for regression
# Compile the model
model_lstm %>% compile(
optimizer = optimizer_adam(clipnorm = 1.0), # Used keras adam oprimizer. Applied clipnorm to limit the gradient value.
loss = "mean_squared_error",  # Simple MSE as a loss function (uniform loss with random forest.)
metrics = c("mean_squared_error")
)
# Check the model summary
summary(model_lstm)
# Check the input data shape
cat("Input Data Shape:", dim(train_data), "\n")
# Train the LSTM Model
model_lstm %>% fit(
x = padded_sequences[trainIndex, , drop = FALSE],
y = processed_df$toxic_score[trainIndex],
epochs = 10,
batch_size = 32,
validation_split = 0.2
)
# AI-X Final Project
# Supervised Learning after doing unsupervised learning.
# Dataset: A chatlog with toxic score assessed by unsupervised learning.
# Column: X(id), Message, Most common report reason, Toxic Score
library(randomForest) # R Package: Random Forest Model
library(keras) # R Package: LSTM Model
library(caret) # R Package:
library(tensorflow) # R Package: Tensorflow
library(reticulate) # R Package for interfacing with python
setwd("~/Desktop/Dev/HYU/2023-02/AI-X/project/gamechatban") # Change this value to your working dir.
#use_virtualenv("/Users/kyumericano/.virtualenvs/r-reticulate/bin/python", required = TRUE)
reticulate::py_config() # Show python interpreter currently configured to use.
# Read a dataset for supervised learning.
processed_df <- read.csv('./offender_chatlog_with_toxic_score.csv')
processed_df <- processed_df[complete.cases(processed_df$toxic_score), ] # Remove rows with missing values
# We are trying to achieve two objective by following supervised learning.
# 1.Build a model that can predict (regress) the toxic level of message.
# 2.Build a model that can classify the most possible common report reason of following chat.
# Objective 1. [Regression Task]: Build a model that can predict the toxic level of message.
# Strategy
# 1. Feature Engineering
# Extract message and toxic score, which are the features required for learning.
# Split data into training data and test data (75% : 25%)
# 2. Build a regression model using random forest model.
# Make regression and evaluate the model.
# Plot the model result.
# 3. Build a regression model using LSTM Model
# Make regression and evaluate the model.
# Plot the model result.
# 1-1 Feature Angineering
# use 'message' column as the feature
messages <- processed_df$message
# Tokenize the message
tokenizer <- text_tokenizer() # Tokenizes text into individual words.
fit_text_tokenizer(tokenizer, messages) # Fits the tokenizer on the input text 'message'
word_idx <- tokenizer$word_index # Retrieves the word index generated by the tokenizer.
sequences <- texts_to_sequences(tokenizer, messages) # Converts the text to a sequence of word indices.
padded_sequences <- pad_sequences(sequences) # Pads sequences to have a consistent length, and it returns a matrix where each row corresponds to a padded sequence.
# Split the data
set.seed(sample(100:1000,1,replace=F)) # Random sampling
trainIndex <- createDataPartition(processed_df$toxic_score, p = 0.75 ,list = FALSE)
train_data <- padded_sequences[trainIndex, ]
test_data <- padded_sequences[-trainIndex, ]
# 1-2: Build a regression model using random forest model. (Random Forest Regressor)
# Random Forest Model
#rf_model <- randomForest(toxic_score ~ ., data = processed_df[trainIndex, c("toxic_score", "message")], ntree = 100)
# Make predictions
#predictions_rf <- predict(rf_model, newdata = processed_df[-trainIndex, c("toxic_score", "message")])
# Evaluate the model
# Evaluate the model via MSE (Mean Squared error)
#mse_rf <- mean((processed_df$toxic_score[-trainIndex] - predictions_rf)^2)
#cat("Random Forest Mean Squared Error:", mse_rf, "\n")
# 1-3: Build a regression model using LSTM model. (Long Short Term Memory Model)
vocab_size <- length(word_idx) # 43268L
seq_num <- ncol(padded_sequences)
# Define the LSTM model
model_lstm <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size, output_dim = 50, input_length = seq_num) %>%
layer_lstm(units = 64, activation = "relu") %>% # Apply ReLU as an activation function to prevent gradient vanishing problem.
layer_dense(units = 1)  # Output layer with a single node for regression
# Compile the model
model_lstm %>% compile(
optimizer = optimizer_adam(clipnorm = 1.0), # Used keras adam oprimizer. Applied clipnorm to limit the gradient value.
loss = "mean_squared_error",  # Simple MSE as a loss function (uniform loss with random forest.)
metrics = c("mean_squared_error")
)
# Check the model summary
summary(model_lstm)
# Check the input data shape
cat("Input Data Shape:", dim(train_data), "\n")
# Train the LSTM Model
model_lstm %>% fit(
x = padded_sequences[trainIndex, , drop = FALSE],
y = processed_df$toxic_score[trainIndex],
epochs = 10,
batch_size = 32,
validation_split = 0.2
)
# Evaluate the model
results_lstm <- model_lstm %>% evaluate(
x = test_data,
y = processed_df$toxic_score[-trainIndex]
)
# Evaluate the model
results_lstm <- model_lstm %>% evaluate(
x = test_data,
y = processed_df$toxic_score[-trainIndex]
)
# Evaluate the model
results_lstm <- model_lstm %>% evaluate(
x = padded_sequences[-trainIndex, , drop = FALSE],
y = processed_df$toxic_score[-trainIndex]
)
# AI-X Final Project
# Supervised Learning after doing unsupervised learning.
# Dataset: A chatlog with toxic score assessed by unsupervised learning.
# Column: X(id), Message, Most common report reason, Toxic Score
library(randomForest) # R Package: Random Forest Model
library(keras) # R Package: LSTM Model
library(caret) # R Package:
library(tensorflow) # R Package: Tensorflow
library(reticulate) # R Package for interfacing with python
setwd("~/Desktop/Dev/HYU/2023-02/AI-X/project/gamechatban") # Change this value to your working dir.
#use_virtualenv("/Users/kyumericano/.virtualenvs/r-reticulate/bin/python", required = TRUE)
reticulate::py_config() # Show python interpreter currently configured to use.
# Read a dataset for supervised learning.
processed_df <- read.csv('./offender_chatlog_with_toxic_score.csv')
processed_df <- processed_df[complete.cases(processed_df$toxic_score), ] # Remove rows with missing values
# We are trying to achieve two objective by following supervised learning.
# 1.Build a model that can predict (regress) the toxic level of message.
# 2.Build a model that can classify the most possible common report reason of following chat.
# Objective 1. [Regression Task]: Build a model that can predict the toxic level of message.
# Strategy
# 1. Feature Engineering
# Extract message and toxic score, which are the features required for learning.
# Split data into training data and test data (75% : 25%)
# 2. Build a regression model using random forest model.
# Make regression and evaluate the model.
# Plot the model result.
# 3. Build a regression model using LSTM Model
# Make regression and evaluate the model.
# Plot the model result.
# 1-1 Feature Angineering
# use 'message' column as the feature
messages <- processed_df$message
# Tokenize the message
tokenizer <- text_tokenizer() # Tokenizes text into individual words.
fit_text_tokenizer(tokenizer, messages) # Fits the tokenizer on the input text 'message'
word_idx <- tokenizer$word_index # Retrieves the word index generated by the tokenizer.
sequences <- texts_to_sequences(tokenizer, messages) # Converts the text to a sequence of word indices.
padded_sequences <- pad_sequences(sequences) # Pads sequences to have a consistent length, and it returns a matrix where each row corresponds to a padded sequence.
# Split the data
set.seed(sample(100:1000,1,replace=F)) # Random sampling
trainIndex <- createDataPartition(processed_df$toxic_score, p = 0.75 ,list = FALSE)
train_data <- padded_sequences[trainIndex, , drop = FALSE]
test_data <- padded_sequences[-trainIndex, ,drop = FALSE]
# 1-2: Build a regression model using random forest model. (Random Forest Regressor)
# Random Forest Model
#rf_model <- randomForest(toxic_score ~ ., data = processed_df[trainIndex, c("toxic_score", "message")], ntree = 100)
# Make predictions
#predictions_rf <- predict(rf_model, newdata = processed_df[-trainIndex, c("toxic_score", "message")])
# Evaluate the model
# Evaluate the model via MSE (Mean Squared error)
#mse_rf <- mean((processed_df$toxic_score[-trainIndex] - predictions_rf)^2)
#cat("Random Forest Mean Squared Error:", mse_rf, "\n")
# 1-3: Build a regression model using LSTM model. (Long Short Term Memory Model)
vocab_size <- length(word_idx) # 43268L
seq_num <- ncol(padded_sequences)
# Define the LSTM model
model_lstm <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size, output_dim = 50, input_length = seq_num) %>%
layer_lstm(units = 64, activation = "relu") %>% # Apply ReLU as an activation function to prevent gradient vanishing problem.
layer_dense(units = 1)  # Output layer with a single node for regression
# Compile the model
model_lstm %>% compile(
optimizer = optimizer_adam(clipnorm = 1.0), # Used keras adam oprimizer. Applied clipnorm to limit the gradient value.
loss = "mean_squared_error",  # Simple MSE as a loss function (uniform loss with random forest.)
metrics = c("mean_squared_error")
)
# Check the model summary
summary(model_lstm)
# Check the input data shape
cat("Input Data Shape:", dim(train_data), "\n")
# Train the LSTM Model
model_lstm %>% fit(
x = train_data,
y = processed_df$toxic_score[trainIndex],
epochs = 10,
batch_size = 32,
validation_split = 0.2
)
# AI-X Final Project
# Supervised Learning after doing unsupervised learning.
# Dataset: A chatlog with toxic score assessed by unsupervised learning.
# Column: X(id), Message, Most common report reason, Toxic Score
library(randomForest) # R Package: Random Forest Model
library(keras) # R Package: LSTM Model
library(caret) # R Package:
library(tensorflow) # R Package: Tensorflow
library(reticulate) # R Package for interfacing with python
setwd("~/Desktop/Dev/HYU/2023-02/AI-X/project/gamechatban") # Change this value to your working dir.
#use_virtualenv("/Users/kyumericano/.virtualenvs/r-reticulate/bin/python", required = TRUE)
reticulate::py_config() # Show python interpreter currently configured to use.
# Read a dataset for supervised learning.
processed_df <- read.csv('./offender_chatlog_with_toxic_score.csv')
processed_df <- processed_df[complete.cases(processed_df$toxic_score), ] # Remove rows with missing values
# We are trying to achieve two objective by following supervised learning.
# 1.Build a model that can predict (regress) the toxic level of message.
# 2.Build a model that can classify the most possible common report reason of following chat.
# Objective 1. [Regression Task]: Build a model that can predict the toxic level of message.
# Strategy
# 1. Feature Engineering
# Extract message and toxic score, which are the features required for learning.
# Split data into training data and test data (75% : 25%)
# 2. Build a regression model using random forest model.
# Make regression and evaluate the model.
# Plot the model result.
# 3. Build a regression model using LSTM Model
# Make regression and evaluate the model.
# Plot the model result.
# 1-1 Feature Angineering
# use 'message' column as the feature
messages <- processed_df$message
# Tokenize the message
tokenizer <- text_tokenizer() # Tokenizes text into individual words.
fit_text_tokenizer(tokenizer, messages) # Fits the tokenizer on the input text 'message'
word_idx <- tokenizer$word_index # Retrieves the word index generated by the tokenizer.
sequences <- texts_to_sequences(tokenizer, messages) # Converts the text to a sequence of word indices.
padded_sequences <- pad_sequences(sequences) # Pads sequences to have a consistent length, and it returns a matrix where each row corresponds to a padded sequence.
# Split the data
set.seed(sample(100:1000,1,replace=F)) # Random sampling
trainIndex <- createDataPartition(processed_df$toxic_score, p = 0.75 ,list = FALSE)
train_data <- padded_sequences[trainIndex, , drop = TRUE]
test_data <- padded_sequences[-trainIndex, ,drop = TRUE]
# 1-2: Build a regression model using random forest model. (Random Forest Regressor)
# Random Forest Model
#rf_model <- randomForest(toxic_score ~ ., data = processed_df[trainIndex, c("toxic_score", "message")], ntree = 100)
# Make predictions
#predictions_rf <- predict(rf_model, newdata = processed_df[-trainIndex, c("toxic_score", "message")])
# Evaluate the model
# Evaluate the model via MSE (Mean Squared error)
#mse_rf <- mean((processed_df$toxic_score[-trainIndex] - predictions_rf)^2)
#cat("Random Forest Mean Squared Error:", mse_rf, "\n")
# 1-3: Build a regression model using LSTM model. (Long Short Term Memory Model)
vocab_size <- length(word_idx) # 43268L
seq_num <- ncol(padded_sequences)
# Define the LSTM model
model_lstm <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size, output_dim = 50, input_length = seq_num) %>%
layer_lstm(units = 64, activation = "relu") %>% # Apply ReLU as an activation function to prevent gradient vanishing problem.
layer_dense(units = 1)  # Output layer with a single node for regression
# Compile the model
model_lstm %>% compile(
optimizer = optimizer_adam(clipnorm = 1.0), # Used keras adam oprimizer. Applied clipnorm to limit the gradient value.
loss = "mean_squared_error",  # Simple MSE as a loss function (uniform loss with random forest.)
metrics = c("mean_squared_error")
)
# Check the model summary
summary(model_lstm)
# Check the input data shape
cat("Input Data Shape:", dim(train_data), "\n")
# Train the LSTM Model
model_lstm %>% fit(
x = train_data,
y = processed_df$toxic_score[trainIndex],
epochs = 10,
batch_size = 32,
validation_split = 0.2
)
# Evaluate the model
results_lstm <- model_lstm %>% evaluate(
x = test_data,
y = processed_df$toxic_score[-trainIndex]
)
# AI-X Final Project
# Supervised Learning after doing unsupervised learning.
# Dataset: A chatlog with toxic score assessed by unsupervised learning.
# Column: X(id), Message, Most common report reason, Toxic Score
library(randomForest) # R Package: Random Forest Model
library(keras) # R Package: LSTM Model
library(caret) # R Package:
library(tensorflow) # R Package: Tensorflow
library(reticulate) # R Package for interfacing with python
setwd("~/Desktop/Dev/HYU/2023-02/AI-X/project/gamechatban") # Change this value to your working dir.
#use_virtualenv("/Users/kyumericano/.virtualenvs/r-reticulate/bin/python", required = TRUE)
reticulate::py_config() # Show python interpreter currently configured to use.
# Read a dataset for supervised learning.
processed_df <- read.csv('./offender_chatlog_with_toxic_score.csv')
processed_df <- processed_df[complete.cases(processed_df$toxic_score), ] # Remove rows with missing values
# We are trying to achieve two objective by following supervised learning.
# 1.Build a model that can predict (regress) the toxic level of message.
# 2.Build a model that can classify the most possible common report reason of following chat.
# Objective 1. [Regression Task]: Build a model that can predict the toxic level of message.
# Strategy
# 1. Feature Engineering
# Extract message and toxic score, which are the features required for learning.
# Split data into training data and test data (75% : 25%)
# 2. Build a regression model using random forest model.
# Make regression and evaluate the model.
# Plot the model result.
# 3. Build a regression model using LSTM Model
# Make regression and evaluate the model.
# Plot the model result.
# 1-1 Feature Angineering
# use 'message' column as the feature
messages <- processed_df$message
# Tokenize the message
tokenizer <- text_tokenizer() # Tokenizes text into individual words.
fit_text_tokenizer(tokenizer, messages) # Fits the tokenizer on the input text 'message'
word_idx <- tokenizer$word_index # Retrieves the word index generated by the tokenizer.
sequences <- texts_to_sequences(tokenizer, messages) # Converts the text to a sequence of word indices.
padded_sequences <- pad_sequences(sequences) # Pads sequences to have a consistent length, and it returns a matrix where each row corresponds to a padded sequence.
# Split the data
set.seed(sample(100:1000,1,replace=F)) # Random sampling
trainIndex <- createDataPartition(processed_df$toxic_score, p = 0.75 ,list = FALSE)
train_data <- padded_sequences[trainIndex, , drop = TRUE]
test_data <- padded_sequences[-trainIndex, ,drop = TRUE]
# 1-2: Build a regression model using random forest model. (Random Forest Regressor)
# Random Forest Model
#rf_model <- randomForest(toxic_score ~ ., data = processed_df[trainIndex, c("toxic_score", "message")], ntree = 100)
# Make predictions
#predictions_rf <- predict(rf_model, newdata = processed_df[-trainIndex, c("toxic_score", "message")])
# Evaluate the model
# Evaluate the model via MSE (Mean Squared error)
#mse_rf <- mean((processed_df$toxic_score[-trainIndex] - predictions_rf)^2)
#cat("Random Forest Mean Squared Error:", mse_rf, "\n")
# 1-3: Build a regression model using LSTM model. (Long Short Term Memory Model)
vocab_size <- length(word_idx) # 43268L
seq_num <- ncol(padded_sequences)
# Define the LSTM model
model_lstm <- keras_model_sequential() %>%
layer_embedding(input_dim = vocab_size, output_dim = 50, input_length = seq_num) %>%
layer_lstm(units = 64, activation = "relu") %>% # Apply ReLU as an activation function to prevent gradient vanishing problem.
layer_dense(units = 1)  # Output layer with a single node for regression
# Compile the model
model_lstm %>% compile(
optimizer = optimizer_adam(clipnorm = 1.0), # Used keras adam oprimizer. Applied clipnorm to limit the gradient value.
loss = "mean_squared_error",  # Simple MSE as a loss function (uniform loss with random forest.)
metrics = c("mean_squared_error")
)
# Check the model summary
summary(model_lstm)
# Check the input data shape
cat("Input Data Shape:", dim(train_data), "\n")
# Train the LSTM Model
model_lstm %>% fit(
x = train_data,
y = processed_df$toxic_score[trainIndex],
epochs = 10,
batch_size = 32,
validation_split = 0.2
)
